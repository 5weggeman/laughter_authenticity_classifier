# Laughter Authenticity Classifier
---
## Copyright Â© 2023 ~ MIT License
---

This repository contains the Python code for a Support Vector Machine (SVM) trained to determine the authenticity of laughter based on its acoustic features.
The classifier functions as a demonstrator for my thesis for the MSc Voice Technology at the University of Groningen:
#### "*The relevance of using authentic laughter data in natural laughter synthesis; A case study on LaughNet*" (2022/2023)
---

### Abstract:

**Purpose:** The purpose of this research was to enhance the naturalness of synthesised speech by incorporating authentic laughter data into the laughter synthesis process of the state-of-the-art model LaughNet (Luong & Yamagishi, 2021).

**Method:** A Support Vector Machine (SVM) was trained to classify acted and spontaneous human laughter based on their acoustic features to confirm the differences between them at the acoustic level. Factor analysis was applied to identify the most relevant acoustic features in determining authenticity. Then the influence of the synthesis procedure of LaughNet on these features was researched by examining the waveform silhouette format and by generating synthetic laughter using LaughNet, classifying it with the SVM, and comparing the classification performance to that of human laughter. The ability of human listeners to recognise the difference between human and synthetic laughter was evaluated using a listening test.

**Results:** The results of this study show that acted and spontaneous laughter can be distinguished on the basis of their acoustic features. The most relevant acoustic features are: 1) the F0 mean, maximum, and variability, 2) the percentage of unvoiced segments and the intensity, and 3) the F0 minimum. Out of these factors, only the second one is captured in the waveform silhouette. The other factors have to be regenerated by the model for the synthetic laughter. This could not be confirmed through synthesis and classification, since I was unable to get sufficiently usable output from LaughNet. Consequently, synthetic laughter could not be evaluated. Human listeners were able to detect the authenticity of human laughter significantly above chance level, with female laughter being easier to classify than male laughter. However, the authenticity judgements were not generally agreed upon.

**Conclusion:** The authenticity of laughter data appears to be irrelevant for the synthesis of natural laughter using LaughNet, as the model generates the most important lower-level acoustic features. However, there is a lack of authentic laughter data for training the model. Future research with sufficient lab-collected data may be able to overcome this limitation by carefully selecting the generative model, data format, and training- and fine-tuning data. Moreover, the perceived authenticity of isolated laughter appears to be contentious, suggesting the need for context to be taken into account in experimental designs as a way to disambiguate the authenticity judgments.

---

## Code description
The code consists of 5 parts:
1. Data preparation
2. Training the classifier
3. Evaluating the classifier
4. Factor analysis
5. Error analysis

In the data preparation, the acoustic features are extracted from the laughter, rid of outliers, and consecutively normalised.
In the training, the data is split into a train and test set, and the hyperparameters of the classifier are fine-tuned using grid search.
In the evaluation, confusion matrices and histograms of projections are created to evaluate the performance of the classifier.
In the factor analysis, it is checked which combinations of acoustic features contributed the most to the decision boundary.
In the error analysis, the acoustic features of misclassified laughter samples are plotted against the respective class means of acted and spontaneous laughter, which can then be cross-referenced with the factor analysis.

Lastly, the sources used in the writing of this code have been listed at the bottom of the code.
